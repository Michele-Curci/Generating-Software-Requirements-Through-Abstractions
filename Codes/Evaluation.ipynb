{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3b842d",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d17aaee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "\n",
    "EXPERIMENT_NAME = \"Dataset_Original\"\n",
    "RESULTS_DIR = f\"results_{EXPERIMENT_NAME}\"\n",
    "\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    os.makedirs(RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39a0056",
   "metadata": {},
   "source": [
    "## 2. Model loading & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76db44e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "except Exception as e:\n",
    "    print(e);\n",
    "\n",
    "# Hierarchy map for relaxed evaluation\n",
    "HIERARCHY_MAP = {\n",
    "    \"Trigger\": \"Condition\",\n",
    "    \"Precondition\": \"Condition\",\n",
    "    \"System_response\": \"Action\",\n",
    "    \"Main_actor\": \"Entity\"\n",
    "}\n",
    "\n",
    "TARGET_COLUMNS = [\n",
    "    'Purpose', 'Condition', 'Main_actor', 'Entity',\n",
    "    'System_response', 'Action', 'Precondition', 'Trigger'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52c77de",
   "metadata": {},
   "source": [
    "## 3. Ground Truth Loader (JSON List Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5f277c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_field(value):\n",
    "    if value is None:\n",
    "        return []\n",
    "    if isinstance(value, list):\n",
    "        return [str(v).strip() for v in value if v and str(v).strip()]\n",
    "    if isinstance(value, str):\n",
    "        return [s.strip() for s in value.split('&-&') if s.strip()]\n",
    "    return []\n",
    "\n",
    "def load_and_normalize_ground_truth(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "    normalized_data = {}\n",
    "\n",
    "    if not isinstance(data, list):\n",
    "        return {}\n",
    "\n",
    "    for item in tqdm(data, desc=\"Parsing JSON\"):\n",
    "        rid = item.get('id')\n",
    "        if rid is None:\n",
    "            continue\n",
    "\n",
    "        row_id = f\"req_{rid}\"\n",
    "\n",
    "        entry = {\n",
    "            'text': item.get('Text', '')\n",
    "        }\n",
    "\n",
    "        for col in TARGET_COLUMNS:\n",
    "            raw_val = item.get(col)\n",
    "            entry[col] = normalize_field(raw_val)\n",
    "\n",
    "        normalized_data[row_id] = entry\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "# --- EXECUTION ---\n",
    "GT_FILENAME = 'requirements.json'\n",
    "\n",
    "ground_truth_data = load_and_normalize_ground_truth(GT_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9251bec9",
   "metadata": {},
   "source": [
    "## 4. Prediction Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc99b91",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ZERO_SHOT_FILE = 'zero_shot_predictions.json'\n",
    "ONE_SHOT_FILE = 'one_shot_predictions.json'\n",
    "FEW_SHOT_FILE = 'few_shot_predictions.json'\n",
    "MULTI_AGENT_FILE = 'multi_agent_predictions.json'\n",
    "MULTI_AGENT_FILE_1 = 'multi_agent_predictions1.json'\n",
    "\n",
    "def load_predictions_json(file_path):\n",
    "    preds = {}\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if isinstance(data, list):\n",
    "            for item in data:\n",
    "                rid = item.get('id')\n",
    "                if rid is None: continue\n",
    "\n",
    "                row_id = f\"req_{rid}\"\n",
    "                clean_entry = {}\n",
    "\n",
    "                source = item.get('prediction', item)\n",
    "\n",
    "                for col in TARGET_COLUMNS:\n",
    "                    raw_val = source.get(col)\n",
    "                    clean_entry[col] = normalize_field(raw_val)\n",
    "\n",
    "                preds[row_id] = clean_entry\n",
    "\n",
    "        return preds\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "# --- EXECUTION ---\n",
    "zero_shot_preds = load_predictions_json(ZERO_SHOT_FILE)\n",
    "if zero_shot_preds:\n",
    "  print(\"zero shot\")\n",
    "one_shot_preds = load_predictions_json(ONE_SHOT_FILE)\n",
    "if one_shot_preds:\n",
    "  print(\"one shot\")\n",
    "few_shot_preds = load_predictions_json(FEW_SHOT_FILE)\n",
    "if few_shot_preds:\n",
    "  print(\"few shot\")\n",
    "multi_agent_preds = load_predictions_json(MULTI_AGENT_FILE)\n",
    "if multi_agent_preds:\n",
    "  print(\"multi\")\n",
    "multi_agent_preds_1 = load_predictions_json(MULTI_AGENT_FILE_1)\n",
    "if multi_agent_preds_1:\n",
    "  print(\"multi_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed08827a",
   "metadata": {},
   "source": [
    "## 5. Core Evaluation Engine (Semantic Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2466741",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_single_prediction(gt_list, pred_list, threshold):\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "\n",
    "    gt_temp = gt_list.copy()\n",
    "    pred_temp = pred_list.copy()\n",
    "\n",
    "    for p in pred_temp[:]:\n",
    "        best_match_idx = -1\n",
    "        best_sim = -1\n",
    "\n",
    "        if p in gt_temp:\n",
    "            best_sim = 1.0\n",
    "            best_match_idx = gt_temp.index(p)\n",
    "        else:\n",
    "            emb_p = sbert_model.encode(p, convert_to_tensor=True)\n",
    "\n",
    "            for i, g in enumerate(gt_temp):\n",
    "                emb_g = sbert_model.encode(g, convert_to_tensor=True)\n",
    "                # Calculate cosine similarity\n",
    "                sim = util.cos_sim(emb_p, emb_g).item()\n",
    "\n",
    "                if sim > best_sim:\n",
    "                    best_sim = sim\n",
    "                    best_match_idx = i\n",
    "\n",
    "        # Check if the best match exceeds the similarity threshold\n",
    "        if best_sim >= threshold and best_match_idx != -1:\n",
    "            tp += 1\n",
    "            gt_temp.pop(best_match_idx) # Remove matched GT item to avoid double counting\n",
    "            pred_temp.remove(p)         # Remove matched Prediction item\n",
    "\n",
    "    # Hallucinations\n",
    "    fp = len(pred_temp)\n",
    "    # Omissions\n",
    "    fn = len(gt_temp)\n",
    "\n",
    "    return tp, fp, fn\n",
    "\n",
    "def run_full_evaluation_optimized(gt_data, pred_data, threshold):\n",
    "    total_tp, total_fp, total_fn = 0, 0, 0\n",
    "\n",
    "    for req_id, gt_entry in gt_data.items():\n",
    "        if req_id not in pred_data:\n",
    "            for col in TARGET_COLUMNS:\n",
    "                total_fn += len(gt_entry.get(col, []))\n",
    "            continue\n",
    "\n",
    "        pred_entry = pred_data[req_id]\n",
    "\n",
    "        for col in TARGET_COLUMNS:\n",
    "            gt_vals = gt_entry.get(col, [])\n",
    "            pred_vals = pred_entry.get(col, [])\n",
    "\n",
    "            tp, fp, fn = evaluate_single_prediction(gt_vals, pred_vals, threshold)\n",
    "\n",
    "            total_tp += tp\n",
    "            total_fp += fp\n",
    "            total_fn += fn\n",
    "\n",
    "    # Calculate Macro Metrics (Safe division)\n",
    "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daec79c",
   "metadata": {},
   "source": [
    "## 6. Threshold Optimization (ROC Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d94cd6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0.15, 0.99, 15)\n",
    "f1_scores = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "if multi_agent_preds_1:\n",
    "    tuning_preds = multi_agent_preds_1\n",
    "    tuning_name = \"Multi-Agent_1\"\n",
    "    print(f\"MultiAgent_1\")\n",
    "elif multi_agent_preds:\n",
    "    tuning_preds = multi_agent_preds\n",
    "    tuning_name = \"Multi-Agent\"\n",
    "    print(f\"MultiAgent\")\n",
    "elif few_shot_preds:\n",
    "    tuning_preds = few_shot_preds\n",
    "    tuning_name = \"Few-Shot\"\n",
    "    print(f\"few-shot\")\n",
    "elif one_shot_preds:\n",
    "    tuning_preds = one_shot_preds\n",
    "    tuning_name = \"One-Shot\"\n",
    "    print(f\"one-shot\")\n",
    "elif zero_shot_preds:\n",
    "    tuning_preds = zero_shot_preds\n",
    "    tuning_name = \"Zero-Shot\"\n",
    "    print(f\"zeroshot\")\n",
    "else:\n",
    "    tuning_preds = {}\n",
    "    tuning_name = \"None\"\n",
    "\n",
    "if tuning_preds:\n",
    "    for t in tqdm(thresholds, desc=\"Testing Thresholds\"):\n",
    "        metrics = run_full_evaluation_optimized(ground_truth_data, tuning_preds, threshold=t)\n",
    "        f1_scores.append(metrics['f1'])\n",
    "        precisions.append(metrics['precision'])\n",
    "        recalls.append(metrics['recall'])\n",
    "\n",
    "    # --- Visualization ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, f1_scores, marker='o', label='F1 Score', linewidth=3, color='#1f77b4')\n",
    "    plt.plot(thresholds, precisions, linestyle='--', label='Precision', color='#2ca02c')\n",
    "    plt.plot(thresholds, recalls, linestyle=':', label='Recall', color='#d62728')\n",
    "\n",
    "    plt.title(f\"Threshold Optimization Analysis ({tuning_name})\", fontsize=14)\n",
    "    plt.xlabel(\"Cosine Similarity Threshold\", fontsize=12)\n",
    "    plt.ylabel(\"Score\", fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    save_path = f\"{RESULTS_DIR}/1_threshold_tuning_{tuning_name}.png\"\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # --- Select Best Threshold ---\n",
    "    if f1_scores:\n",
    "        best_idx = np.argmax(f1_scores)\n",
    "        best_threshold = thresholds[best_idx]\n",
    "        print(f\"Optimal Threshold Found: {best_threshold:.2f} (Max F1: {f1_scores[best_idx]:.3f})\")\n",
    "    else:\n",
    "        best_threshold = 0.70\n",
    "else:\n",
    "    best_threshold = 0.70"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b559abf",
   "metadata": {},
   "source": [
    "## 7. Comparative Report & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69283da",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_exact_match_score(gt_data, pred_data):\n",
    "    total_tp, total_fp, total_fn = 0, 0, 0\n",
    "\n",
    "    for req_id, gt_entry in gt_data.items():\n",
    "        if req_id not in pred_data:\n",
    "            for col in TARGET_COLUMNS:\n",
    "                total_fn += len(gt_entry.get(col, []))\n",
    "            continue\n",
    "\n",
    "        pred_entry = pred_data[req_id]\n",
    "\n",
    "        for col in TARGET_COLUMNS:\n",
    "            gt_vals = [s.lower().strip() for s in gt_entry.get(col, [])]\n",
    "            pred_vals = [s.lower().strip() for s in pred_entry.get(col, [])]\n",
    "\n",
    "            current_tp = 0\n",
    "            for p in pred_vals[:]:\n",
    "                if p in gt_vals:\n",
    "                    current_tp += 1\n",
    "                    gt_vals.remove(p)\n",
    "                    pred_vals.remove(p)\n",
    "\n",
    "            total_tp += current_tp\n",
    "            total_fp += len(pred_vals)\n",
    "            total_fn += len(gt_vals)\n",
    "\n",
    "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "def calculate_category_breakdown(gt_data, pred_data, threshold):\n",
    "    breakdown = {}\n",
    "    for col in TARGET_COLUMNS:\n",
    "        total_tp, total_fp, total_fn = 0, 0, 0\n",
    "        for req_id, gt_entry in gt_data.items():\n",
    "            if req_id not in pred_data:\n",
    "                total_fn += len(gt_entry.get(col, []))\n",
    "                continue\n",
    "            gt_vals = gt_entry.get(col, [])\n",
    "            pred_vals = pred_data[req_id].get(col, [])\n",
    "            # Use SBERT\n",
    "            tp, fp, fn = evaluate_single_prediction(gt_vals, pred_vals, threshold)\n",
    "            total_tp += tp; total_fp += fp; total_fn += fn\n",
    "\n",
    "        p = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "        r = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "        f1 = 2 * (p * r) / (p + r) if (p + r) > 0 else 0\n",
    "        breakdown[col] = f1\n",
    "    return breakdown\n",
    "\n",
    "potential_models = [\n",
    "    {\n",
    "        \"id\": \"zero_shot\",\n",
    "        \"data\": zero_shot_preds,\n",
    "        \"name\": \"Llama-3 Zero-Shot\",\n",
    "        \"strategy\": \"Baseline\",\n",
    "        \"color\": \"#444444\" # Grey\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"one_shot\",\n",
    "        \"data\": one_shot_preds,\n",
    "        \"name\": \"Llama-3 One-Shot\",\n",
    "        \"strategy\": \"In-Context Learning\",\n",
    "        \"color\": \"#FF991C\" # Red/Orange\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"few_shot\",\n",
    "        \"data\": few_shot_preds,\n",
    "        \"name\": \"Llama-3 Few-Shot\",\n",
    "        \"strategy\": \"In-Context (5-Shot)\",\n",
    "        \"color\": \"#1f77b4\" # Blue\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"multi_agent\",\n",
    "        \"data\": multi_agent_preds,\n",
    "        \"name\": \"Llama-3 Multi-Agent\",\n",
    "        \"strategy\": \"Agentic Workflow\",\n",
    "        \"color\": \"#55aa22\" # Green\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"multi_agent_1\",\n",
    "        \"data\": multi_agent_preds_1,\n",
    "        \"name\": \"Llama-3 Multi-Agent_1\",\n",
    "        \"strategy\": \"Agentic Workflow\",\n",
    "        \"color\": \"#fd3db5\" # Magenta\n",
    "    }\n",
    "]\n",
    "\n",
    "results_list = []\n",
    "colors_used = []\n",
    "\n",
    "best_model_data = {}\n",
    "best_model_name = \"\"\n",
    "max_f1 = -1\n",
    "\n",
    "for model in potential_models:\n",
    "    if model[\"data\"]:\n",
    "        metrics = run_full_evaluation_optimized(ground_truth_data, model[\"data\"], threshold=best_threshold)\n",
    "\n",
    "        results_list.append({\n",
    "            \"Model Architecture\": model[\"name\"],\n",
    "            \"Strategy\": model[\"strategy\"],\n",
    "            \"Precision\": metrics['precision'],\n",
    "            \"Recall\": metrics['recall'],\n",
    "            \"F1-Score\": metrics['f1']\n",
    "        })\n",
    "        colors_used.append(model[\"color\"])\n",
    "\n",
    "        if metrics['f1'] > max_f1:\n",
    "            max_f1 = metrics['f1']\n",
    "            best_model_data = model[\"data\"]\n",
    "            best_model_name = model[\"name\"]\n",
    "\n",
    "    else:\n",
    "        print(f\"No data loaded\")\n",
    "\n",
    "if results_list:\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    print(\"\\nEXPERIMENTAL RESULTS SUMMARY\")\n",
    "\n",
    "    display(results_df.style.background_gradient(cmap='Greens', subset=['F1-Score']))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(results_df['Model Architecture'], results_df['F1-Score'], color=colors_used)\n",
    "\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2.0, height,\n",
    "                 f'{height:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "    plt.title('F1-Score Comparison by Architecture', fontsize=14)\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.xticks(rotation=0)\n",
    "\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/2_model_comparison_f1.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    if best_model_data:\n",
    "        print(f\"{best_model_name}\")\n",
    "\n",
    "        exact_f1 = calculate_exact_match_score(ground_truth_data, best_model_data)\n",
    "        sbert_f1 = max_f1\n",
    "\n",
    "        print(f\"\\n1. WHY SBERT?\")\n",
    "        print(f\"Exact String Match F1:  {exact_f1:.3f}\")\n",
    "        print(f\"SBERT Semantic F1:      {sbert_f1:.3f}\")\n",
    "        print(f\"- Improvement: +{(sbert_f1 - exact_f1)*100:.1f}% thanks to semantic evaluation.\")\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.bar(['Exact Match', 'SBERT AI'], [exact_f1, sbert_f1], color=['#999999', '#1f77b4'])\n",
    "        plt.title(\"Impact of Semantic Evaluation Metrics\")\n",
    "        plt.ylabel(\"F1 Score\")\n",
    "        plt.ylim(0, 1.0)\n",
    "        for i, v in enumerate([exact_f1, sbert_f1]):\n",
    "            plt.text(i, v + 0.02, f\"{v:.3f}\", ha='center', fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{RESULTS_DIR}/3_exact_vs_semantic.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        print(f\"\\n2. PERFORMANCE BY CATEGORY\")\n",
    "        cat_scores = calculate_category_breakdown(ground_truth_data, best_model_data, best_threshold)\n",
    "        sorted_cats = dict(sorted(cat_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        colors_cat = plt.cm.viridis(np.linspace(0, 0.9, len(sorted_cats)))\n",
    "        c_bars = plt.bar(sorted_cats.keys(), sorted_cats.values(), color=colors_cat)\n",
    "        plt.title(f\"F1-Score by Category ({best_model_name})\")\n",
    "        plt.ylabel(\"F1 Score\")\n",
    "        plt.ylim(0, 1.1)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "        for bar in c_bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                     f'{height:.2f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{RESULTS_DIR}/4_category_breakdown.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"check Loader.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e45b7d0",
   "metadata": {},
   "source": [
    "## 8. Qualitative Error Analysis\n",
    "Exports specific examples of hallucinations and omissions to validate metrics with human review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d85b98",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def analyze_qualitative_errors(gt_data, pred_data, threshold, num_examples=2, model_name=\"Model\"):\n",
    "    report_lines = []\n",
    "\n",
    "    header = f\"QUALITATIVE ANALYSIS REPORT: {model_name}\\n\" + \"=\"*60\n",
    "    print(header)\n",
    "    report_lines.append(header)\n",
    "\n",
    "    for col in TARGET_COLUMNS:\n",
    "        errors_found = 0\n",
    "\n",
    "        section_header = f\"\\n\\nCategory: {col}\\n\" + \"-\"*40\n",
    "        print(section_header)\n",
    "        report_lines.append(section_header)\n",
    "\n",
    "        for req_id, gt_entry in gt_data.items():\n",
    "            if errors_found >= num_examples: break\n",
    "\n",
    "            if req_id not in pred_data: continue\n",
    "\n",
    "            gt_vals = gt_entry.get(col, [])\n",
    "            pred_vals = pred_data.get(req_id, {}).get(col, [])\n",
    "\n",
    "            tp, fp, fn = evaluate_single_prediction(gt_vals, pred_vals, threshold)\n",
    "\n",
    "            if fp > 0 or fn > 0:\n",
    "\n",
    "                error_block = []\n",
    "                error_block.append(f\"\\n[ID: {req_id}]\")\n",
    "\n",
    "                text_content = gt_entry.get('text', '')\n",
    "                text_preview = text_content[:80] + \"...\" if len(text_content) > 80 else text_content\n",
    "                print(f\"Context: \\\"{text_preview}\\\"\")\n",
    "                error_block.append(f\"Context: \\\"{text_preview}\\\"\")\n",
    "                error_block.append(f\"Ground Truth: {gt_vals}\")\n",
    "                error_block.append(f\"Prediction:   {pred_vals}\")\n",
    "\n",
    "                if fp > 0:\n",
    "                  error_block.append(f\"   Type: HALLUCINATION (Extracted info not in GT)\")\n",
    "                if fn > 0:\n",
    "                  error_block.append(f\"   Type: OMISSION (Missed info present in GT)\")\n",
    "\n",
    "                print(\"-\" * 40)\n",
    "\n",
    "                msg = \"\\n\".join(error_block)\n",
    "                report_lines.append(msg)\n",
    "                print(msg)\n",
    "\n",
    "                errors_found += 1\n",
    "\n",
    "        if errors_found == 0:\n",
    "            msg = \"\\n   (No errors found in this category for the checked samples)\"\n",
    "            print(msg)\n",
    "            report_lines.append(msg)\n",
    "\n",
    "    filename = f\"qualitative_errors_{model_name.replace(' ', '_')}.txt\"\n",
    "    save_path = os.path.join(RESULTS_DIR, filename)\n",
    "\n",
    "    try:\n",
    "        with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(report_lines))\n",
    "    except Exception as e:\n",
    "        print(f\"{e}\")\n",
    "\n",
    "# --- EXECUTION ---\n",
    "\n",
    "target_preds = {}\n",
    "target_name = \"\"\n",
    "\n",
    "if multi_agent_preds_1:\n",
    "    target_preds = multi_agent_preds_1\n",
    "    target_name = \"Multi-Agent_1\"\n",
    "elif multi_agent_preds:\n",
    "    target_preds = multi_agent_preds\n",
    "    target_name = \"Multi-Agent\"\n",
    "elif few_shot_preds:\n",
    "    target_preds = few_shot_preds\n",
    "    target_name = \"Few-Shot\"\n",
    "elif one_shot_preds:\n",
    "    target_preds = one_shot_preds\n",
    "    target_name = \"One-Shot\"\n",
    "elif zero_shot_preds:\n",
    "    target_preds = zero_shot_preds\n",
    "    target_name = \"Zero-Shot\"\n",
    "\n",
    "if target_preds:\n",
    "    analyze_qualitative_errors(ground_truth_data, target_preds, best_threshold, num_examples=3, model_name=target_name)\n",
    "else:\n",
    "    print(\"No predictions available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e9937",
   "metadata": {},
   "source": [
    "## 9. Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c69a421",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def check_semantic_match_visual(text1, text2, threshold):\n",
    "    if not text1 or not text2: return False\n",
    "\n",
    "    emb1 = sbert_model.encode(text1, convert_to_tensor=True)\n",
    "    emb2 = sbert_model.encode(text2, convert_to_tensor=True)\n",
    "\n",
    "    return util.cos_sim(emb1, emb2).item() >= threshold\n",
    "\n",
    "def plot_dual_confusion_matrix(gt_data, pred_data, threshold, model_name=\"Model\"):\n",
    "    cols = TARGET_COLUMNS\n",
    "    matrix = np.zeros((len(cols), len(cols)))\n",
    "\n",
    "    print(f\"Confusion matrix for {model_name}...\")\n",
    "\n",
    "    for req_id, gt_entry in tqdm(gt_data.items(), desc=\"Processing Matrix\"):\n",
    "        if req_id not in pred_data: continue\n",
    "        pred_entry = pred_data[req_id]\n",
    "\n",
    "        for i, gt_col in enumerate(cols):\n",
    "            gt_vals = gt_entry.get(gt_col, [])\n",
    "            for gt_val in gt_vals:\n",
    "                match_found_in_col = -1\n",
    "\n",
    "                for j, pred_col in enumerate(cols):\n",
    "                    pred_vals = pred_entry.get(pred_col, [])\n",
    "                    for p_val in pred_vals:\n",
    "                        if check_semantic_match_visual(gt_val, p_val, threshold):\n",
    "                            match_found_in_col = j\n",
    "                            break\n",
    "                    if match_found_in_col != -1: break\n",
    "\n",
    "                if match_found_in_col != -1:\n",
    "                    matrix[i, match_found_in_col] += 1\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(matrix, annot=True, fmt='g', xticklabels=cols, yticklabels=cols, cmap='Oranges', cbar=False)\n",
    "    plt.title(f\"Confusion Matrix (RAW COUNTS) - {model_name}\", fontsize=14, fontweight='bold', pad=15)\n",
    "    plt.xlabel(\"Predicted Category\", fontsize=11)\n",
    "    plt.ylabel(\"Ground Truth Category\", fontsize=11)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/5_confusion_matrix_raw_{model_name}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    row_sums = matrix.sum(axis=1)[:, np.newaxis]\n",
    "    norm_matrix = matrix / (row_sums + 1e-10) # Epsilon to aviod div/0\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(norm_matrix, annot=True, fmt='.1%', xticklabels=cols, yticklabels=cols, cmap='Blues', vmin=0, vmax=1)\n",
    "    plt.title(f\"Confusion Matrix (NORMALIZED) - {model_name}\", fontsize=14, fontweight='bold', pad=15)\n",
    "    plt.xlabel(\"Predicted Category\", fontsize=11)\n",
    "    plt.ylabel(\"Ground Truth Category\", fontsize=11)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/6_confusion_matrix_norm_{model_name}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "MODEL_TO_ANALYZE = 'multi_1' # zero || one || few || multi || multi_1\n",
    "\n",
    "target_preds = {}\n",
    "target_name = \"\"\n",
    "\n",
    "if MODEL_TO_ANALYZE == 'multi_1' and multi_agent_preds_1:\n",
    "    target_preds = multi_agent_preds_1\n",
    "    target_name = \"Multi-Agent_1\"\n",
    "elif MODEL_TO_ANALYZE == 'multi' and multi_agent_preds:\n",
    "    target_preds = multi_agent_preds\n",
    "    target_name = \"Multi-Agent\"\n",
    "elif MODEL_TO_ANALYZE == 'few' and few_shot_preds:\n",
    "    target_preds = few_shot_preds\n",
    "    target_name = \"Few-Shot\"\n",
    "elif MODEL_TO_ANALYZE == 'one' and one_shot_preds:\n",
    "    target_preds = one_shot_preds\n",
    "    target_name = \"One-Shot\"\n",
    "elif zero_shot_preds:\n",
    "    target_preds = zero_shot_preds\n",
    "    target_name = \"Zero-Shot\"\n",
    "\n",
    "if target_preds:\n",
    "    plot_dual_confusion_matrix(ground_truth_data, target_preds, best_threshold, target_name)\n",
    "else:\n",
    "    print(\"No data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b88975",
   "metadata": {},
   "source": [
    "## 10. Final Executive Summary & Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85231e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Total times taken from Time.docx\n",
    "total_time_zero = 2456.34\n",
    "total_time_one = 3217.84\n",
    "total_time_few = 3188.60\n",
    "total_time_multi = 3198.90\n",
    "total_time_multi_1 = 5218.89\n",
    "\n",
    "num_reqs = len(ground_truth_data) if 'ground_truth_data' in globals() and ground_truth_data else 254\n",
    "\n",
    "avg_time_zero = total_time_zero / num_reqs\n",
    "avg_time_one = total_time_one / num_reqs\n",
    "avg_time_few = total_time_few / num_reqs\n",
    "avg_time_multi = total_time_multi / num_reqs\n",
    "avg_time_multi_1 = total_time_multi_1 / num_reqs\n",
    "\n",
    "models_to_plot = []\n",
    "times_to_plot = []\n",
    "f1_to_plot = []\n",
    "\n",
    "\n",
    "def get_f1(model_name_part):\n",
    "    if 'results_df' not in globals(): return None\n",
    "    try:\n",
    "        row = results_df[results_df['Model Architecture'].str.contains(model_name_part, case=False, na=False)]\n",
    "        if not row.empty:\n",
    "            return row['F1-Score'].values[0]\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# Data Retrieval (Logic: if F1 exists, add the model to the plot)\n",
    "f1_z = get_f1(\"Zero-Shot\")\n",
    "if f1_z is not None:\n",
    "    models_to_plot.append(\"Zero-Shot\")\n",
    "    times_to_plot.append(avg_time_zero)\n",
    "    f1_to_plot.append(f1_z)\n",
    "\n",
    "f1_o = get_f1(\"One-Shot\")\n",
    "if f1_o is not None:\n",
    "    models_to_plot.append(\"One-Shot\")\n",
    "    times_to_plot.append(avg_time_one)\n",
    "    f1_to_plot.append(f1_o)\n",
    "\n",
    "f1_f = get_f1(\"Few-Shot\")\n",
    "if f1_f is not None:\n",
    "    models_to_plot.append(\"Few-Shot\")\n",
    "    times_to_plot.append(avg_time_few)\n",
    "    f1_to_plot.append(f1_f)\n",
    "\n",
    "f1_m = get_f1(\"Multi-Agent\")\n",
    "if f1_m is not None:\n",
    "    models_to_plot.append(\"Multi-Agent\")\n",
    "    times_to_plot.append(avg_time_multi)\n",
    "    f1_to_plot.append(f1_m)\n",
    "\n",
    "f1_m1 = get_f1(\"Multi-Agent_1\")\n",
    "if f1_m1 is not None:\n",
    "    models_to_plot.append(\"Multi-Agent_1\")\n",
    "    times_to_plot.append(avg_time_multi_1)\n",
    "    f1_to_plot.append(f1_m1)\n",
    "\n",
    "if models_to_plot:\n",
    "    color_f1 = '#4682B4'\n",
    "    color_time = '#B22222'\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    ax1.set_xlabel('Architecture', fontsize=12, fontweight='bold', labelpad=10)\n",
    "    ax1.set_ylabel('F1 Score', color=color_f1, fontsize=12, fontweight='bold')\n",
    "\n",
    "    bars = ax1.bar(models_to_plot, f1_to_plot, color=color_f1, alpha=0.6, width=0.5, label='F1 Score')\n",
    "\n",
    "    max_f1 = max(f1_to_plot) if f1_to_plot else 1.0\n",
    "    ax1.set_ylim(0, max_f1 * 1.2)\n",
    "    ax1.tick_params(axis='y', labelcolor=color_f1)\n",
    "    ax1.grid(visible=True, axis='y', linestyle='--', alpha=0.3) # Light grid for F1 only\n",
    "\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if height > 0:\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                     f'{height:.3f}',\n",
    "                     ha='center', va='bottom', color=color_f1, fontweight='bold', fontsize=11,\n",
    "                     bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1))\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Avg Latency (s/req)', color=color_time, fontsize=12, fontweight='bold')\n",
    "\n",
    "    ax2.plot(models_to_plot, times_to_plot, color=color_time, marker='D', markersize=8, linewidth=2.5, label='Latency', zorder=10)\n",
    "\n",
    "    # Dynamic scale for Time\n",
    "    max_time = max(times_to_plot) if times_to_plot else 10\n",
    "    ax2.set_ylim(0, max_time * 1.3)\n",
    "    ax2.tick_params(axis='y', labelcolor=color_time)\n",
    "\n",
    "    for i, txt in enumerate(times_to_plot):\n",
    "        ax2.text(i, txt, f'{txt:.1f}s',\n",
    "                 ha='center', va='bottom', color=color_time, fontweight='bold', fontsize=11,\n",
    "                 bbox=dict(facecolor='white', alpha=0.8, edgecolor='none', pad=1))\n",
    "\n",
    "    plt.title(\"Model Quality vs. Computational Cost\", fontsize=14, pad=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/7_tradeoff_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# --- 4. TEXTUAL REPORT ---\n",
    "\n",
    "report_lines = []\n",
    "report_lines.append(\"=\"*60)\n",
    "report_lines.append(\"FINAL EXECUTIVE SUMMARY\")\n",
    "report_lines.append(\"=\"*60)\n",
    "\n",
    "report_lines.append(\"\\n1. PERFORMANCE METRICS (Quality)\")\n",
    "if 'results_df' in globals():\n",
    "    report_lines.append(results_df[['Model Architecture', 'Precision', 'Recall', 'F1-Score']].to_string(index=False))\n",
    "else:\n",
    "    report_lines.append(\"No metric data available.\")\n",
    "\n",
    "report_lines.append(f\"\\n2. OPTIMIZATION PARAMETERS\")\n",
    "if 'best_threshold' in globals():\n",
    "    report_lines.append(f\"- Optimal Semantic Threshold (SBERT): {best_threshold:.2f}\")\n",
    "\n",
    "report_lines.append(\"\\n3. COST/BENEFIT ANALYSIS\")\n",
    "try:\n",
    "    if len(f1_to_plot) >= 2:\n",
    "        # Compare Best vs Baseline\n",
    "        base_idx = 0\n",
    "        best_idx = np.argmax(f1_to_plot)\n",
    "\n",
    "        base_name = models_to_plot[base_idx]\n",
    "        best_name = models_to_plot[best_idx]\n",
    "\n",
    "        f1_gain = ((f1_to_plot[best_idx] - f1_to_plot[base_idx]) / f1_to_plot[base_idx]) * 100\n",
    "\n",
    "        if times_to_plot[base_idx] > 0:\n",
    "            time_change = ((times_to_plot[best_idx] - times_to_plot[base_idx]) / times_to_plot[base_idx]) * 100\n",
    "        else:\n",
    "            time_change = 0.0\n",
    "\n",
    "        trend_time = \"increase\" if time_change > 0 else \"decrease\"\n",
    "\n",
    "        report_lines.append(f\"- Best Model (Quality): {best_name}\")\n",
    "        report_lines.append(f\"- Quality Impact: Moving from {base_name} to {best_name}\")\n",
    "        report_lines.append(f\"  resulted in an F1-Score improvement of {f1_gain:+.1f}%.\")\n",
    "\n",
    "        if abs(time_change) > 0.1:\n",
    "            report_lines.append(f\"- Cost Impact: This resulted in a latency {trend_time} of {abs(time_change):.1f}%\")\n",
    "            report_lines.append(f\"  ({times_to_plot[base_idx]:.1f}s -> {times_to_plot[best_idx]:.1f}s per requirement).\")\n",
    "        else:\n",
    "            report_lines.append(f\"- Cost Impact: Latency data unavailable or identical.\")\n",
    "\n",
    "    else:\n",
    "        report_lines.append(\"Insufficient data for comparative trade-off calculation.\")\n",
    "\n",
    "except Exception as e:\n",
    "    report_lines.append(f\"Note: Unable to calculate detailed trade-off ({e})\")\n",
    "\n",
    "report_lines.append(\"=\"*60)\n",
    "\n",
    "# Uniamo tutto in un unico testo\n",
    "full_report_text = \"\\n\".join(report_lines)\n",
    "\n",
    "# A. Stampa a schermo\n",
    "print(full_report_text)\n",
    "\n",
    "# B. Salva su file TXT\n",
    "report_path = f\"{RESULTS_DIR}/executive_summary.txt\"\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(full_report_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30407bc",
   "metadata": {},
   "source": [
    "## 11. Final Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f923b7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(EXPERIMENT_NAME)\n",
    "print(RESULTS_DIR)\n",
    "\n",
    "if 'results_df' in globals():\n",
    "    # CSV\n",
    "    csv_path = f\"{RESULTS_DIR}/summary_metrics.csv\"\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    # LaTeX\n",
    "    latex_path = f\"{RESULTS_DIR}/latex_table.txt\"\n",
    "    with open(latex_path, \"w\") as f:\n",
    "        f.write(results_df.to_latex(index=False, float_format=\"%.3f\"))\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    best_model_key = None\n",
    "    if 'multi_agent_preds_1' in globals() and multi_agent_preds_1: best_model_key = multi_agent_preds_1\n",
    "    elif 'multi_agent_preds' in globals() and multi_agent_preds: best_model_key = multi_agent_preds\n",
    "    elif 'few_shot_preds' in globals() and few_shot_preds: best_model_key = few_shot_preds\n",
    "    elif 'one_shot_preds' in globals() and one_shot_preds: best_model_key = one_shot_preds\n",
    "\n",
    "    if best_model_key:\n",
    "        detailed_rows = []\n",
    "\n",
    "        for req_id, gt_entry in ground_truth_data.items():\n",
    "            if req_id in best_model_key:\n",
    "                pred_entry = best_model_key[req_id]\n",
    "\n",
    "                gt_all = []; pred_all = []\n",
    "                for col in TARGET_COLUMNS:\n",
    "                    gt_all.extend(gt_entry.get(col, []))\n",
    "                    pred_all.extend(pred_entry.get(col, []))\n",
    "\n",
    "                tp, fp, fn = evaluate_single_prediction(gt_all, pred_all, best_threshold)\n",
    "\n",
    "                p = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "                r = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "                f1 = 2 * (p * r) / (p + r) if (p + r) > 0 else 0\n",
    "\n",
    "                detailed_rows.append({\n",
    "                    \"Req_ID\": req_id,\n",
    "                    \"F1_Score\": f1,\n",
    "                    \"Precision\": p,\n",
    "                    \"Recall\": r,\n",
    "                    \"GT_Items\": len(gt_all),\n",
    "                    \"Pred_Items\": len(pred_all)\n",
    "                })\n",
    "\n",
    "        det_path = f\"{RESULTS_DIR}/detailed_analysis_per_req.csv\"\n",
    "        pd.DataFrame(detailed_rows).to_csv(det_path, index=False)\n",
    "        print(f\"saved to: {det_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "zip_filename = f\"results_{EXPERIMENT_NAME}\"\n",
    "shutil.make_archive(zip_filename, 'zip', RESULTS_DIR)\n",
    "\n",
    "print(f\"\\n ready {zip_filename}.zip\")\n",
    "files.download(f\"{zip_filename}.zip\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
